{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Full models Grid search.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3hHMYCougmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "from google.colab import drive # remove later\n",
        "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Reshape, Conv2D, MaxPooling2D \n",
        "from tensorflow.keras.layers import CuDNNLSTM, Dropout, BatchNormalization\n",
        "%matplotlib inline\n",
        "\n",
        "### ------------------------------------------------------------------------ ###\n",
        "# Create time stamp\n",
        "start_time = datetime.now()\n",
        "print(\"=\"*80)\n",
        "print(\"Script Execution initilize at\", start_time)\n",
        "print(\"=\"*80,\"\\n\\n\")\n",
        "### ------------------------------------------------------------------------ ###\n",
        "\n",
        "# Mounting Google Drive, remove later\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Load dataset\n",
        "feat = np.load(\"/content/drive/My Drive/ML Group Project Folder/src/feat.npy\", allow_pickle = True)\n",
        "path = np.load(\"/content/drive/My Drive/ML Group Project Folder/src/path.npy\")\n",
        "train = pd.read_csv(\"/content/drive/My Drive/ML Group Project Folder/src/train.csv\")\n",
        "test = pd.read_csv(\"/content/drive/My Drive/ML Group Project Folder/src/test.csv\")\n",
        "\n",
        "# Merge MFCC footprint with Path\n",
        "df = pd.DataFrame({'path': path, 'MFCC': feat})\n",
        "\n",
        "# Tag path in train and test dataset with MFCC Footprint\n",
        "train_container = train.merge(df, on=['path'])\n",
        "test_container = test.merge(df, on=['path'])\n",
        "\n",
        "# Remove column Path:\n",
        "# Now, each dataset would only contain MFCC Footprint and word label\n",
        "train_set = train_container.drop(columns=[\"path\"])\n",
        "test_set = test_container.drop(columns=[\"path\"])\n",
        "\n",
        "# Convert features and corresponding classification labels into numpy arrays\n",
        "X = np.array(train_set.MFCC.tolist())\n",
        "y = np.array(train_set.word.tolist())\n",
        "\n",
        "# Encode the classification labels\n",
        "le = LabelEncoder()\n",
        "class_label = le.fit_transform(y)\n",
        "\n",
        "# Transform the data\n",
        "n, t, c = X.shape[0], X[0].shape[0], X[0][0].shape[0]\n",
        "container_X = np.zeros(shape=(n, t, c))\n",
        "for i in range(n):\n",
        " for j in range(t):\n",
        "   for k in range(c):\n",
        "     try:\n",
        "       container_X[i, j, k] = X[i][j][k]\n",
        "     except:\n",
        "        pass\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(container_X, class_label,\n",
        "                                                  test_size=0.2,\n",
        "                                                  random_state = 24,\n",
        "                                                  stratify = class_label)\n",
        "def grid_search(batch, lr):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(BatchNormalization(input_shape=(X_train.shape[1:])))\n",
        "    model.add(Reshape((99,13,1)))\n",
        "\n",
        "    model.add(Conv2D(filters=256, kernel_size=(4,4), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(2,2), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Conv2D(filters=32, kernel_size=(2,2), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Reshape((44,32)))\n",
        "    model.add(CuDNNLSTM(256))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(35, activation = \"softmax\"))\n",
        "\n",
        "    # Compile the model\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate= lr, decay=1e-5)\n",
        "    model.compile(loss = \"sparse_categorical_crossentropy\",\n",
        "                  optimizer = opt,\n",
        "                  metrics = [\"sparse_categorical_accuracy\"])\n",
        "\n",
        "    # Display model architecture summary \n",
        "    model.summary()\n",
        "\n",
        "    # Calculate pre-training accuracy \n",
        "    score = model.evaluate(X_val, y_val, verbose=1)\n",
        "    accuracy = 100*score[1]\n",
        "\n",
        "    print(\"Pre-training accuracy: %.4f%%\" % accuracy)\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "    ### ------------------------------------------------------------------------ ###\n",
        "    # Create time stamp\n",
        "    cp1 = datetime.now() - start_time\n",
        "    print(\"=\"*80)\n",
        "    print(\"Time elapsed:\", cp1)\n",
        "    print(\"Training is about to begin\")\n",
        "    print(\"=\"*80,\"\\n\\n\")\n",
        "    cp1 = datetime.now()\n",
        "    ### ------------------------------------------------------------------------ ###\n",
        "\n",
        "\n",
        "    ### ------------------------------------------------------------------------ ###\n",
        "    ### ------------------------   Training the Model   ------------------------ ###\n",
        "    ### ------------------------------------------------------------------------ ###\n",
        "\n",
        "    # Define Callbacks\n",
        "    cb = [ModelCheckpoint(\n",
        "        filepath=\"/content/drive/My Drive/out/Conv2D LSTM Stratify Model Kwastas 1 CP.hdf5\",\n",
        "        verbose=1, save_best_only=True), #change path\n",
        "          EarlyStopping(monitor = \"val_loss\", patience = 10, \n",
        "                        mode = \"min\", verbose = 1)]\n",
        "\n",
        "    # For logging exec time\n",
        "    start = datetime.now()\n",
        "\n",
        "    # Fit model\n",
        "    model_result = model.fit(X_train, y_train, epochs = 1000, batch_size = batch,\n",
        "                             validation_data=(X_val, y_val), \n",
        "                             callbacks=cb, verbose=1)\n",
        "\n",
        "    # For displaying time\n",
        "    duration = datetime.now() - start\n",
        "    print(\"Training completed in time: \", duration)\n",
        "\n",
        "    # Save current model\n",
        "    model.save(\"/content/drive/My Drive/out/Conv2D LSTM Stratify Model Kwastas 1.h5\")\n",
        "    print(\"Model is saved in /content/drive/My Drive/out/Conv2D LSTM Stratify Model Kwastas 1.h5\") # change later\n",
        "\n",
        "    # Model Evaluation\n",
        "    # Load the best weight model\n",
        "    model.load_weights(\"/content/drive/My Drive/out/Conv2D LSTM Stratify Model Kwastas 1 CP.hdf5\")\n",
        "    _, train_acc = model.evaluate(X_train, y_train, verbose=1)\n",
        "    _, val_acc = model.evaluate(X_val, y_val, verbose=1)\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Evaluation on the best model\")\n",
        "    print('Train: %.3f \\nTest: %.3f' % (train_acc, val_acc))\n",
        "    print(\"=\"*80, \"\\n\")\n",
        "\n",
        "    # Plotting model_result\n",
        "    plt.plot(model_result.history['sparse_categorical_accuracy'], label='train')\n",
        "    plt.plot(model_result.history['val_sparse_categorical_accuracy'], label='test')\n",
        "    plt.title(\"Model Accuracy\")\n",
        "    plt.legend([\"train\", \"test\"], loc = \"lower right\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "    print(\"=\"*80, \"\\n\")\n",
        "\n",
        "\n",
        "    plt.plot(model_result.history['loss'], label='train')\n",
        "    plt.plot(model_result.history['val_loss'], label='test')\n",
        "    plt.title(\"Model Error\")\n",
        "    plt.legend([\"train\", \"test\"], loc = \"upper right\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Error\")\n",
        "    plt.show()\n",
        "\n",
        "    hyperparameters = (lr, batch)\n",
        "    result[hyperparameters] = (val_acc)\n",
        "    print (result)\n",
        "\n",
        "\n",
        "learning_rate = [0.001, 0.0001]\n",
        "batch_size = [32,64]\n",
        "result = {}\n",
        "\n",
        "for batch in batch_size:\n",
        "  for lr in learning_rate:\n",
        "    grid_search(batch, lr)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}